{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Digit Recognizer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYDonYNwNxXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pytorch utility imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "#neural net imports\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XllTDi5Mivb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import external libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import math\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MlTBSug6oBk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c7c760d6-ee62-4a73-92bb-2cc1e3e5fe85"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.backends.cudnn.enabled)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkR3Ugb78Gm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_folder_path = \"/content/drive/My Drive/Data Set/\"\n",
        "train_df = pd.read_csv(input_folder_path+\"train.csv\")\n",
        "test_df = pd.read_csv(input_folder_path+\"test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJBE7RtV857C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = train_df['label'].values\n",
        "train_images = (train_df.iloc[:,1:].values).astype('float32')\n",
        "test_images = (test_df.iloc[:,:].values).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqCH1uw6k35L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, stratify=train_labels, random_state=123, test_size=0.20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ZZGOgtA0UU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28)\n",
        "val_images = val_images.reshape(val_images.shape[0], 28, 28)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6RqPemtBF5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "ded79664-84f1-4f2d-a0cf-9e7464210360"
      },
      "source": [
        "for i in range(106, 109):\n",
        "    plt.subplot(330 + (i+1))\n",
        "    plt.imshow(train_images[i], cmap=plt.get_cmap('gray'))\n",
        "    plt.title(train_labels[i])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAABeCAYAAAC9+abLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nav8H6qJDFtB7h14AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO5BKLANBO7G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "5f45721f-4481-4a36-ae95-09653eb9258c"
      },
      "source": [
        "for i in range(26, 29):\n",
        "    plt.subplot(330 + (i+1))\n",
        "    plt.imshow(test_images[i].squeeze(), cmap=plt.get_cmap('gray'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOkAAABcCAYAAACLMJIlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb94qdG1BjjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train\n",
        "train_images_tensor = torch.tensor(train_images)/255.0\n",
        "train_labels_tensor = torch.tensor(train_labels)\n",
        "train_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "\n",
        "#val\n",
        "val_images_tensor = torch.tensor(val_images)/255.0\n",
        "val_labels_tensor = torch.tensor(val_labels)\n",
        "val_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "\n",
        "#test\n",
        "test_images_tensor = torch.tensor(test_images)/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcCpHv3GB1FO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_tensor, batch_size=16, num_workers=2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvTD2315B4k1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loader = DataLoader(val_tensor, batch_size=16, num_workers=2, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKfSeeezB6c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = DataLoader(test_images_tensor, batch_size=16, num_workers=2, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzaPXXlPCArp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "9671e92c-4e39-4286-831a-eeac5114a2a4"
      },
      "source": [
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    img_grid = make_grid(data[0:8,].unsqueeze(1), nrow=8)\n",
        "    img_target_labels = target[0:8,].numpy()\n",
        "    break\n",
        "    \n",
        "plt.imshow(img_grid.numpy().transpose((1,2,0)))\n",
        "plt.rcParams['figure.figsize'] = (10, 2)\n",
        "plt.title(img_target_labels, size=16)\n",
        "plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if s != self._text:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABdCAYAAAC1t50ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwRAiGIVuMBgMIYJR6AaDwRAi/D9p8Nf21GNhdAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlAhKdlaCca4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) \n",
        "        )\n",
        "        \n",
        "        self.linear_block = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(128*7*7, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_block(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLvR6bU_Cr4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "b70fecd6-3019-44e1-df60-42b6871f6e3f"
      },
      "source": [
        "conv_model = Net()\n",
        "conv_model"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv_block): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace)\n",
              "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (linear_block): Sequential(\n",
              "    (0): Dropout(p=0.5)\n",
              "    (1): Linear(in_features=6272, out_features=128, bias=True)\n",
              "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU(inplace)\n",
              "    (4): Dropout(p=0.5)\n",
              "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU(inplace)\n",
              "    (8): Dropout(p=0.5)\n",
              "    (9): Linear(in_features=64, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hH0NM-AD4Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(params=conv_model.parameters(), lr=0.003)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    conv_model = conv_model.cuda()\n",
        "    criterion = criterion.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8t6oqOpEB8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(num_epoch):\n",
        "    conv_model.train()\n",
        "    exp_lr_scheduler.step()\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.unsqueeze(1)\n",
        "        data, target = data, target\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        output = conv_model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (batch_idx + 1)% 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                num_epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
        "                100. * (batch_idx + 1) / len(train_loader), loss.data))\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2t-2r0vELSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(data_loader):\n",
        "    conv_model.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for data, target in data_loader:\n",
        "        data = data.unsqueeze(1)\n",
        "        data, target = data, target\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "        \n",
        "        output = conv_model(data)\n",
        "        \n",
        "        loss += F.cross_entropy(output, target, size_average=False).data\n",
        "\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        \n",
        "    loss /= len(data_loader.dataset)\n",
        "        \n",
        "    print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
        "        loss, correct, len(data_loader.dataset),\n",
        "        100. * correct / len(data_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff68VyOeENbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30e827ca-4a15-4705-968f-685bcead211c"
      },
      "source": [
        "num_epochs = 25\n",
        "\n",
        "for n in range(num_epochs):\n",
        "    train_model(n)\n",
        "    evaluate(val_loader)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [1600/33600 (5%)]\tLoss: 1.007431\n",
            "Train Epoch: 0 [3200/33600 (10%)]\tLoss: 0.500019\n",
            "Train Epoch: 0 [4800/33600 (14%)]\tLoss: 0.921573\n",
            "Train Epoch: 0 [6400/33600 (19%)]\tLoss: 1.280389\n",
            "Train Epoch: 0 [8000/33600 (24%)]\tLoss: 0.590223\n",
            "Train Epoch: 0 [9600/33600 (29%)]\tLoss: 0.391682\n",
            "Train Epoch: 0 [11200/33600 (33%)]\tLoss: 0.386440\n",
            "Train Epoch: 0 [12800/33600 (38%)]\tLoss: 0.558033\n",
            "Train Epoch: 0 [14400/33600 (43%)]\tLoss: 0.062788\n",
            "Train Epoch: 0 [16000/33600 (48%)]\tLoss: 0.443590\n",
            "Train Epoch: 0 [17600/33600 (52%)]\tLoss: 0.209644\n",
            "Train Epoch: 0 [19200/33600 (57%)]\tLoss: 0.373792\n",
            "Train Epoch: 0 [20800/33600 (62%)]\tLoss: 0.644702\n",
            "Train Epoch: 0 [22400/33600 (67%)]\tLoss: 0.148443\n",
            "Train Epoch: 0 [24000/33600 (71%)]\tLoss: 0.979266\n",
            "Train Epoch: 0 [25600/33600 (76%)]\tLoss: 0.560252\n",
            "Train Epoch: 0 [27200/33600 (81%)]\tLoss: 0.228923\n",
            "Train Epoch: 0 [28800/33600 (86%)]\tLoss: 0.511846\n",
            "Train Epoch: 0 [30400/33600 (90%)]\tLoss: 0.112271\n",
            "Train Epoch: 0 [32000/33600 (95%)]\tLoss: 0.223381\n",
            "Train Epoch: 0 [33600/33600 (100%)]\tLoss: 0.658979\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Average Val Loss: 0.0626, Val Accuracy: 8253/8400 (98.000%)\n",
            "\n",
            "Train Epoch: 1 [1600/33600 (5%)]\tLoss: 0.122161\n",
            "Train Epoch: 1 [3200/33600 (10%)]\tLoss: 0.854534\n",
            "Train Epoch: 1 [4800/33600 (14%)]\tLoss: 0.064457\n",
            "Train Epoch: 1 [6400/33600 (19%)]\tLoss: 0.039919\n",
            "Train Epoch: 1 [8000/33600 (24%)]\tLoss: 0.064947\n",
            "Train Epoch: 1 [9600/33600 (29%)]\tLoss: 0.228423\n",
            "Train Epoch: 1 [11200/33600 (33%)]\tLoss: 0.353575\n",
            "Train Epoch: 1 [12800/33600 (38%)]\tLoss: 0.150201\n",
            "Train Epoch: 1 [14400/33600 (43%)]\tLoss: 0.261913\n",
            "Train Epoch: 1 [16000/33600 (48%)]\tLoss: 0.538308\n",
            "Train Epoch: 1 [17600/33600 (52%)]\tLoss: 0.140236\n",
            "Train Epoch: 1 [19200/33600 (57%)]\tLoss: 0.129935\n",
            "Train Epoch: 1 [20800/33600 (62%)]\tLoss: 0.277349\n",
            "Train Epoch: 1 [22400/33600 (67%)]\tLoss: 0.291613\n",
            "Train Epoch: 1 [24000/33600 (71%)]\tLoss: 0.028287\n",
            "Train Epoch: 1 [25600/33600 (76%)]\tLoss: 0.357403\n",
            "Train Epoch: 1 [27200/33600 (81%)]\tLoss: 0.115038\n",
            "Train Epoch: 1 [28800/33600 (86%)]\tLoss: 0.032310\n",
            "Train Epoch: 1 [30400/33600 (90%)]\tLoss: 0.037329\n",
            "Train Epoch: 1 [32000/33600 (95%)]\tLoss: 0.052959\n",
            "Train Epoch: 1 [33600/33600 (100%)]\tLoss: 0.042963\n",
            "\n",
            "Average Val Loss: 0.0457, Val Accuracy: 8282/8400 (98.000%)\n",
            "\n",
            "Train Epoch: 2 [1600/33600 (5%)]\tLoss: 0.271877\n",
            "Train Epoch: 2 [3200/33600 (10%)]\tLoss: 0.170609\n",
            "Train Epoch: 2 [4800/33600 (14%)]\tLoss: 0.008212\n",
            "Train Epoch: 2 [6400/33600 (19%)]\tLoss: 0.070314\n",
            "Train Epoch: 2 [8000/33600 (24%)]\tLoss: 0.793016\n",
            "Train Epoch: 2 [9600/33600 (29%)]\tLoss: 0.062535\n",
            "Train Epoch: 2 [11200/33600 (33%)]\tLoss: 0.030701\n",
            "Train Epoch: 2 [12800/33600 (38%)]\tLoss: 0.099400\n",
            "Train Epoch: 2 [14400/33600 (43%)]\tLoss: 0.030053\n",
            "Train Epoch: 2 [16000/33600 (48%)]\tLoss: 0.543615\n",
            "Train Epoch: 2 [17600/33600 (52%)]\tLoss: 0.102587\n",
            "Train Epoch: 2 [19200/33600 (57%)]\tLoss: 0.054115\n",
            "Train Epoch: 2 [20800/33600 (62%)]\tLoss: 0.020195\n",
            "Train Epoch: 2 [22400/33600 (67%)]\tLoss: 0.074703\n",
            "Train Epoch: 2 [24000/33600 (71%)]\tLoss: 0.052549\n",
            "Train Epoch: 2 [25600/33600 (76%)]\tLoss: 0.770695\n",
            "Train Epoch: 2 [27200/33600 (81%)]\tLoss: 0.024877\n",
            "Train Epoch: 2 [28800/33600 (86%)]\tLoss: 0.065719\n",
            "Train Epoch: 2 [30400/33600 (90%)]\tLoss: 0.083815\n",
            "Train Epoch: 2 [32000/33600 (95%)]\tLoss: 0.044846\n",
            "Train Epoch: 2 [33600/33600 (100%)]\tLoss: 0.102840\n",
            "\n",
            "Average Val Loss: 0.0434, Val Accuracy: 8292/8400 (98.000%)\n",
            "\n",
            "Train Epoch: 3 [1600/33600 (5%)]\tLoss: 0.019194\n",
            "Train Epoch: 3 [3200/33600 (10%)]\tLoss: 0.031123\n",
            "Train Epoch: 3 [4800/33600 (14%)]\tLoss: 0.041837\n",
            "Train Epoch: 3 [6400/33600 (19%)]\tLoss: 0.056887\n",
            "Train Epoch: 3 [8000/33600 (24%)]\tLoss: 0.105604\n",
            "Train Epoch: 3 [9600/33600 (29%)]\tLoss: 0.082194\n",
            "Train Epoch: 3 [11200/33600 (33%)]\tLoss: 0.035609\n",
            "Train Epoch: 3 [12800/33600 (38%)]\tLoss: 0.291834\n",
            "Train Epoch: 3 [14400/33600 (43%)]\tLoss: 0.108869\n",
            "Train Epoch: 3 [16000/33600 (48%)]\tLoss: 0.131584\n",
            "Train Epoch: 3 [17600/33600 (52%)]\tLoss: 0.126107\n",
            "Train Epoch: 3 [19200/33600 (57%)]\tLoss: 0.118269\n",
            "Train Epoch: 3 [20800/33600 (62%)]\tLoss: 0.004219\n",
            "Train Epoch: 3 [22400/33600 (67%)]\tLoss: 0.157676\n",
            "Train Epoch: 3 [24000/33600 (71%)]\tLoss: 0.004626\n",
            "Train Epoch: 3 [25600/33600 (76%)]\tLoss: 0.278319\n",
            "Train Epoch: 3 [27200/33600 (81%)]\tLoss: 0.145497\n",
            "Train Epoch: 3 [28800/33600 (86%)]\tLoss: 0.056678\n",
            "Train Epoch: 3 [30400/33600 (90%)]\tLoss: 0.304863\n",
            "Train Epoch: 3 [32000/33600 (95%)]\tLoss: 0.132592\n",
            "Train Epoch: 3 [33600/33600 (100%)]\tLoss: 0.032514\n",
            "\n",
            "Average Val Loss: 0.0389, Val Accuracy: 8313/8400 (98.000%)\n",
            "\n",
            "Train Epoch: 4 [1600/33600 (5%)]\tLoss: 0.060331\n",
            "Train Epoch: 4 [3200/33600 (10%)]\tLoss: 0.109177\n",
            "Train Epoch: 4 [4800/33600 (14%)]\tLoss: 0.014391\n",
            "Train Epoch: 4 [6400/33600 (19%)]\tLoss: 0.109394\n",
            "Train Epoch: 4 [8000/33600 (24%)]\tLoss: 0.241792\n",
            "Train Epoch: 4 [9600/33600 (29%)]\tLoss: 0.053636\n",
            "Train Epoch: 4 [11200/33600 (33%)]\tLoss: 0.134224\n",
            "Train Epoch: 4 [12800/33600 (38%)]\tLoss: 0.011518\n",
            "Train Epoch: 4 [14400/33600 (43%)]\tLoss: 0.066330\n",
            "Train Epoch: 4 [16000/33600 (48%)]\tLoss: 0.015042\n",
            "Train Epoch: 4 [17600/33600 (52%)]\tLoss: 0.084268\n",
            "Train Epoch: 4 [19200/33600 (57%)]\tLoss: 0.043395\n",
            "Train Epoch: 4 [20800/33600 (62%)]\tLoss: 0.094594\n",
            "Train Epoch: 4 [22400/33600 (67%)]\tLoss: 1.203813\n",
            "Train Epoch: 4 [24000/33600 (71%)]\tLoss: 0.319138\n",
            "Train Epoch: 4 [25600/33600 (76%)]\tLoss: 0.083379\n",
            "Train Epoch: 4 [27200/33600 (81%)]\tLoss: 0.104408\n",
            "Train Epoch: 4 [28800/33600 (86%)]\tLoss: 0.054364\n",
            "Train Epoch: 4 [30400/33600 (90%)]\tLoss: 0.061490\n",
            "Train Epoch: 4 [32000/33600 (95%)]\tLoss: 0.006183\n",
            "Train Epoch: 4 [33600/33600 (100%)]\tLoss: 0.033386\n",
            "\n",
            "Average Val Loss: 0.0410, Val Accuracy: 8308/8400 (98.000%)\n",
            "\n",
            "Train Epoch: 5 [1600/33600 (5%)]\tLoss: 0.089144\n",
            "Train Epoch: 5 [3200/33600 (10%)]\tLoss: 0.070965\n",
            "Train Epoch: 5 [4800/33600 (14%)]\tLoss: 0.044344\n",
            "Train Epoch: 5 [6400/33600 (19%)]\tLoss: 0.050937\n",
            "Train Epoch: 5 [8000/33600 (24%)]\tLoss: 0.036949\n",
            "Train Epoch: 5 [9600/33600 (29%)]\tLoss: 0.023713\n",
            "Train Epoch: 5 [11200/33600 (33%)]\tLoss: 0.016618\n",
            "Train Epoch: 5 [12800/33600 (38%)]\tLoss: 0.191974\n",
            "Train Epoch: 5 [14400/33600 (43%)]\tLoss: 0.106490\n",
            "Train Epoch: 5 [16000/33600 (48%)]\tLoss: 0.036298\n",
            "Train Epoch: 5 [17600/33600 (52%)]\tLoss: 0.044214\n",
            "Train Epoch: 5 [19200/33600 (57%)]\tLoss: 0.096096\n",
            "Train Epoch: 5 [20800/33600 (62%)]\tLoss: 0.004936\n",
            "Train Epoch: 5 [22400/33600 (67%)]\tLoss: 0.410740\n",
            "Train Epoch: 5 [24000/33600 (71%)]\tLoss: 0.053105\n",
            "Train Epoch: 5 [25600/33600 (76%)]\tLoss: 0.194181\n",
            "Train Epoch: 5 [27200/33600 (81%)]\tLoss: 0.022371\n",
            "Train Epoch: 5 [28800/33600 (86%)]\tLoss: 0.597837\n",
            "Train Epoch: 5 [30400/33600 (90%)]\tLoss: 0.255545\n",
            "Train Epoch: 5 [32000/33600 (95%)]\tLoss: 0.039587\n",
            "Train Epoch: 5 [33600/33600 (100%)]\tLoss: 0.021761\n",
            "\n",
            "Average Val Loss: 0.0304, Val Accuracy: 8324/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 6 [1600/33600 (5%)]\tLoss: 0.168540\n",
            "Train Epoch: 6 [3200/33600 (10%)]\tLoss: 0.093377\n",
            "Train Epoch: 6 [4800/33600 (14%)]\tLoss: 0.014379\n",
            "Train Epoch: 6 [6400/33600 (19%)]\tLoss: 0.007611\n",
            "Train Epoch: 6 [8000/33600 (24%)]\tLoss: 0.211151\n",
            "Train Epoch: 6 [9600/33600 (29%)]\tLoss: 0.093170\n",
            "Train Epoch: 6 [11200/33600 (33%)]\tLoss: 0.026055\n",
            "Train Epoch: 6 [12800/33600 (38%)]\tLoss: 0.008765\n",
            "Train Epoch: 6 [14400/33600 (43%)]\tLoss: 0.003782\n",
            "Train Epoch: 6 [16000/33600 (48%)]\tLoss: 0.041124\n",
            "Train Epoch: 6 [17600/33600 (52%)]\tLoss: 0.082520\n",
            "Train Epoch: 6 [19200/33600 (57%)]\tLoss: 0.063220\n",
            "Train Epoch: 6 [20800/33600 (62%)]\tLoss: 0.178574\n",
            "Train Epoch: 6 [22400/33600 (67%)]\tLoss: 0.016165\n",
            "Train Epoch: 6 [24000/33600 (71%)]\tLoss: 0.068695\n",
            "Train Epoch: 6 [25600/33600 (76%)]\tLoss: 0.319725\n",
            "Train Epoch: 6 [27200/33600 (81%)]\tLoss: 0.035622\n",
            "Train Epoch: 6 [28800/33600 (86%)]\tLoss: 0.239444\n",
            "Train Epoch: 6 [30400/33600 (90%)]\tLoss: 0.123669\n",
            "Train Epoch: 6 [32000/33600 (95%)]\tLoss: 0.020689\n",
            "Train Epoch: 6 [33600/33600 (100%)]\tLoss: 0.126769\n",
            "\n",
            "Average Val Loss: 0.0292, Val Accuracy: 8329/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 7 [1600/33600 (5%)]\tLoss: 0.019869\n",
            "Train Epoch: 7 [3200/33600 (10%)]\tLoss: 0.003447\n",
            "Train Epoch: 7 [4800/33600 (14%)]\tLoss: 0.032782\n",
            "Train Epoch: 7 [6400/33600 (19%)]\tLoss: 0.053881\n",
            "Train Epoch: 7 [8000/33600 (24%)]\tLoss: 0.050095\n",
            "Train Epoch: 7 [9600/33600 (29%)]\tLoss: 0.404057\n",
            "Train Epoch: 7 [11200/33600 (33%)]\tLoss: 0.069065\n",
            "Train Epoch: 7 [12800/33600 (38%)]\tLoss: 0.048703\n",
            "Train Epoch: 7 [14400/33600 (43%)]\tLoss: 0.359906\n",
            "Train Epoch: 7 [16000/33600 (48%)]\tLoss: 0.078813\n",
            "Train Epoch: 7 [17600/33600 (52%)]\tLoss: 0.002520\n",
            "Train Epoch: 7 [19200/33600 (57%)]\tLoss: 0.031919\n",
            "Train Epoch: 7 [20800/33600 (62%)]\tLoss: 0.029918\n",
            "Train Epoch: 7 [22400/33600 (67%)]\tLoss: 0.261693\n",
            "Train Epoch: 7 [24000/33600 (71%)]\tLoss: 0.003251\n",
            "Train Epoch: 7 [25600/33600 (76%)]\tLoss: 0.007668\n",
            "Train Epoch: 7 [27200/33600 (81%)]\tLoss: 0.008629\n",
            "Train Epoch: 7 [28800/33600 (86%)]\tLoss: 0.010761\n",
            "Train Epoch: 7 [30400/33600 (90%)]\tLoss: 0.035355\n",
            "Train Epoch: 7 [32000/33600 (95%)]\tLoss: 0.024527\n",
            "Train Epoch: 7 [33600/33600 (100%)]\tLoss: 0.023837\n",
            "\n",
            "Average Val Loss: 0.0283, Val Accuracy: 8333/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 8 [1600/33600 (5%)]\tLoss: 0.254443\n",
            "Train Epoch: 8 [3200/33600 (10%)]\tLoss: 0.234988\n",
            "Train Epoch: 8 [4800/33600 (14%)]\tLoss: 0.004224\n",
            "Train Epoch: 8 [6400/33600 (19%)]\tLoss: 0.052522\n",
            "Train Epoch: 8 [8000/33600 (24%)]\tLoss: 0.015019\n",
            "Train Epoch: 8 [9600/33600 (29%)]\tLoss: 0.006087\n",
            "Train Epoch: 8 [11200/33600 (33%)]\tLoss: 0.019931\n",
            "Train Epoch: 8 [12800/33600 (38%)]\tLoss: 0.013874\n",
            "Train Epoch: 8 [14400/33600 (43%)]\tLoss: 0.031540\n",
            "Train Epoch: 8 [16000/33600 (48%)]\tLoss: 0.049501\n",
            "Train Epoch: 8 [17600/33600 (52%)]\tLoss: 0.195103\n",
            "Train Epoch: 8 [19200/33600 (57%)]\tLoss: 0.009790\n",
            "Train Epoch: 8 [20800/33600 (62%)]\tLoss: 0.033336\n",
            "Train Epoch: 8 [22400/33600 (67%)]\tLoss: 0.038125\n",
            "Train Epoch: 8 [24000/33600 (71%)]\tLoss: 0.075038\n",
            "Train Epoch: 8 [25600/33600 (76%)]\tLoss: 0.011231\n",
            "Train Epoch: 8 [27200/33600 (81%)]\tLoss: 0.075724\n",
            "Train Epoch: 8 [28800/33600 (86%)]\tLoss: 0.556225\n",
            "Train Epoch: 8 [30400/33600 (90%)]\tLoss: 0.018627\n",
            "Train Epoch: 8 [32000/33600 (95%)]\tLoss: 0.005073\n",
            "Train Epoch: 8 [33600/33600 (100%)]\tLoss: 0.075706\n",
            "\n",
            "Average Val Loss: 0.0284, Val Accuracy: 8335/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 9 [1600/33600 (5%)]\tLoss: 0.013424\n",
            "Train Epoch: 9 [3200/33600 (10%)]\tLoss: 0.000736\n",
            "Train Epoch: 9 [4800/33600 (14%)]\tLoss: 0.007603\n",
            "Train Epoch: 9 [6400/33600 (19%)]\tLoss: 0.160557\n",
            "Train Epoch: 9 [8000/33600 (24%)]\tLoss: 0.006521\n",
            "Train Epoch: 9 [9600/33600 (29%)]\tLoss: 0.077379\n",
            "Train Epoch: 9 [11200/33600 (33%)]\tLoss: 0.009065\n",
            "Train Epoch: 9 [12800/33600 (38%)]\tLoss: 0.002738\n",
            "Train Epoch: 9 [14400/33600 (43%)]\tLoss: 0.028079\n",
            "Train Epoch: 9 [16000/33600 (48%)]\tLoss: 0.013487\n",
            "Train Epoch: 9 [17600/33600 (52%)]\tLoss: 0.013962\n",
            "Train Epoch: 9 [19200/33600 (57%)]\tLoss: 0.049770\n",
            "Train Epoch: 9 [20800/33600 (62%)]\tLoss: 0.003266\n",
            "Train Epoch: 9 [22400/33600 (67%)]\tLoss: 0.127344\n",
            "Train Epoch: 9 [24000/33600 (71%)]\tLoss: 0.052710\n",
            "Train Epoch: 9 [25600/33600 (76%)]\tLoss: 0.051128\n",
            "Train Epoch: 9 [27200/33600 (81%)]\tLoss: 0.002097\n",
            "Train Epoch: 9 [28800/33600 (86%)]\tLoss: 0.003500\n",
            "Train Epoch: 9 [30400/33600 (90%)]\tLoss: 0.022349\n",
            "Train Epoch: 9 [32000/33600 (95%)]\tLoss: 0.001182\n",
            "Train Epoch: 9 [33600/33600 (100%)]\tLoss: 0.013349\n",
            "\n",
            "Average Val Loss: 0.0280, Val Accuracy: 8333/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 10 [1600/33600 (5%)]\tLoss: 0.012677\n",
            "Train Epoch: 10 [3200/33600 (10%)]\tLoss: 0.038773\n",
            "Train Epoch: 10 [4800/33600 (14%)]\tLoss: 0.446075\n",
            "Train Epoch: 10 [6400/33600 (19%)]\tLoss: 0.087410\n",
            "Train Epoch: 10 [8000/33600 (24%)]\tLoss: 0.110257\n",
            "Train Epoch: 10 [9600/33600 (29%)]\tLoss: 0.012629\n",
            "Train Epoch: 10 [11200/33600 (33%)]\tLoss: 0.024583\n",
            "Train Epoch: 10 [12800/33600 (38%)]\tLoss: 0.038031\n",
            "Train Epoch: 10 [14400/33600 (43%)]\tLoss: 0.003734\n",
            "Train Epoch: 10 [16000/33600 (48%)]\tLoss: 0.022229\n",
            "Train Epoch: 10 [17600/33600 (52%)]\tLoss: 0.013978\n",
            "Train Epoch: 10 [19200/33600 (57%)]\tLoss: 0.012357\n",
            "Train Epoch: 10 [20800/33600 (62%)]\tLoss: 0.348511\n",
            "Train Epoch: 10 [22400/33600 (67%)]\tLoss: 1.165072\n",
            "Train Epoch: 10 [24000/33600 (71%)]\tLoss: 0.008538\n",
            "Train Epoch: 10 [25600/33600 (76%)]\tLoss: 0.253786\n",
            "Train Epoch: 10 [27200/33600 (81%)]\tLoss: 0.031947\n",
            "Train Epoch: 10 [28800/33600 (86%)]\tLoss: 0.014671\n",
            "Train Epoch: 10 [30400/33600 (90%)]\tLoss: 0.049264\n",
            "Train Epoch: 10 [32000/33600 (95%)]\tLoss: 0.159468\n",
            "Train Epoch: 10 [33600/33600 (100%)]\tLoss: 0.040295\n",
            "\n",
            "Average Val Loss: 0.0276, Val Accuracy: 8338/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 11 [1600/33600 (5%)]\tLoss: 0.050703\n",
            "Train Epoch: 11 [3200/33600 (10%)]\tLoss: 0.007864\n",
            "Train Epoch: 11 [4800/33600 (14%)]\tLoss: 0.042948\n",
            "Train Epoch: 11 [6400/33600 (19%)]\tLoss: 0.028754\n",
            "Train Epoch: 11 [8000/33600 (24%)]\tLoss: 0.140668\n",
            "Train Epoch: 11 [9600/33600 (29%)]\tLoss: 0.011811\n",
            "Train Epoch: 11 [11200/33600 (33%)]\tLoss: 0.005137\n",
            "Train Epoch: 11 [12800/33600 (38%)]\tLoss: 0.004727\n",
            "Train Epoch: 11 [14400/33600 (43%)]\tLoss: 0.061985\n",
            "Train Epoch: 11 [16000/33600 (48%)]\tLoss: 0.301747\n",
            "Train Epoch: 11 [17600/33600 (52%)]\tLoss: 0.193578\n",
            "Train Epoch: 11 [19200/33600 (57%)]\tLoss: 0.038373\n",
            "Train Epoch: 11 [20800/33600 (62%)]\tLoss: 0.102759\n",
            "Train Epoch: 11 [22400/33600 (67%)]\tLoss: 0.019305\n",
            "Train Epoch: 11 [24000/33600 (71%)]\tLoss: 0.005475\n",
            "Train Epoch: 11 [25600/33600 (76%)]\tLoss: 0.069994\n",
            "Train Epoch: 11 [27200/33600 (81%)]\tLoss: 0.468821\n",
            "Train Epoch: 11 [28800/33600 (86%)]\tLoss: 0.004880\n",
            "Train Epoch: 11 [30400/33600 (90%)]\tLoss: 0.005099\n",
            "Train Epoch: 11 [32000/33600 (95%)]\tLoss: 0.002017\n",
            "Train Epoch: 11 [33600/33600 (100%)]\tLoss: 0.043194\n",
            "\n",
            "Average Val Loss: 0.0273, Val Accuracy: 8336/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 12 [1600/33600 (5%)]\tLoss: 0.093388\n",
            "Train Epoch: 12 [3200/33600 (10%)]\tLoss: 0.219239\n",
            "Train Epoch: 12 [4800/33600 (14%)]\tLoss: 0.015069\n",
            "Train Epoch: 12 [6400/33600 (19%)]\tLoss: 0.001271\n",
            "Train Epoch: 12 [8000/33600 (24%)]\tLoss: 0.236932\n",
            "Train Epoch: 12 [9600/33600 (29%)]\tLoss: 0.110530\n",
            "Train Epoch: 12 [11200/33600 (33%)]\tLoss: 0.032381\n",
            "Train Epoch: 12 [12800/33600 (38%)]\tLoss: 0.004225\n",
            "Train Epoch: 12 [14400/33600 (43%)]\tLoss: 0.138123\n",
            "Train Epoch: 12 [16000/33600 (48%)]\tLoss: 0.345841\n",
            "Train Epoch: 12 [17600/33600 (52%)]\tLoss: 0.004530\n",
            "Train Epoch: 12 [19200/33600 (57%)]\tLoss: 0.008093\n",
            "Train Epoch: 12 [20800/33600 (62%)]\tLoss: 0.202136\n",
            "Train Epoch: 12 [22400/33600 (67%)]\tLoss: 0.017758\n",
            "Train Epoch: 12 [24000/33600 (71%)]\tLoss: 0.012816\n",
            "Train Epoch: 12 [25600/33600 (76%)]\tLoss: 0.118255\n",
            "Train Epoch: 12 [27200/33600 (81%)]\tLoss: 0.005793\n",
            "Train Epoch: 12 [28800/33600 (86%)]\tLoss: 0.021843\n",
            "Train Epoch: 12 [30400/33600 (90%)]\tLoss: 0.006755\n",
            "Train Epoch: 12 [32000/33600 (95%)]\tLoss: 0.116872\n",
            "Train Epoch: 12 [33600/33600 (100%)]\tLoss: 0.058797\n",
            "\n",
            "Average Val Loss: 0.0287, Val Accuracy: 8336/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 13 [1600/33600 (5%)]\tLoss: 0.065034\n",
            "Train Epoch: 13 [3200/33600 (10%)]\tLoss: 0.011297\n",
            "Train Epoch: 13 [4800/33600 (14%)]\tLoss: 0.008296\n",
            "Train Epoch: 13 [6400/33600 (19%)]\tLoss: 0.014826\n",
            "Train Epoch: 13 [8000/33600 (24%)]\tLoss: 0.052232\n",
            "Train Epoch: 13 [9600/33600 (29%)]\tLoss: 0.249043\n",
            "Train Epoch: 13 [11200/33600 (33%)]\tLoss: 0.154198\n",
            "Train Epoch: 13 [12800/33600 (38%)]\tLoss: 0.000633\n",
            "Train Epoch: 13 [14400/33600 (43%)]\tLoss: 0.015519\n",
            "Train Epoch: 13 [16000/33600 (48%)]\tLoss: 0.029146\n",
            "Train Epoch: 13 [17600/33600 (52%)]\tLoss: 0.076105\n",
            "Train Epoch: 13 [19200/33600 (57%)]\tLoss: 0.324503\n",
            "Train Epoch: 13 [20800/33600 (62%)]\tLoss: 0.174923\n",
            "Train Epoch: 13 [22400/33600 (67%)]\tLoss: 0.003301\n",
            "Train Epoch: 13 [24000/33600 (71%)]\tLoss: 0.012506\n",
            "Train Epoch: 13 [25600/33600 (76%)]\tLoss: 0.167875\n",
            "Train Epoch: 13 [27200/33600 (81%)]\tLoss: 0.022074\n",
            "Train Epoch: 13 [28800/33600 (86%)]\tLoss: 0.001926\n",
            "Train Epoch: 13 [30400/33600 (90%)]\tLoss: 0.010032\n",
            "Train Epoch: 13 [32000/33600 (95%)]\tLoss: 0.083025\n",
            "Train Epoch: 13 [33600/33600 (100%)]\tLoss: 0.017213\n",
            "\n",
            "Average Val Loss: 0.0265, Val Accuracy: 8338/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 14 [1600/33600 (5%)]\tLoss: 0.114911\n",
            "Train Epoch: 14 [3200/33600 (10%)]\tLoss: 0.111568\n",
            "Train Epoch: 14 [4800/33600 (14%)]\tLoss: 0.069586\n",
            "Train Epoch: 14 [6400/33600 (19%)]\tLoss: 0.134961\n",
            "Train Epoch: 14 [8000/33600 (24%)]\tLoss: 0.696531\n",
            "Train Epoch: 14 [9600/33600 (29%)]\tLoss: 0.001971\n",
            "Train Epoch: 14 [11200/33600 (33%)]\tLoss: 0.191181\n",
            "Train Epoch: 14 [12800/33600 (38%)]\tLoss: 0.049890\n",
            "Train Epoch: 14 [14400/33600 (43%)]\tLoss: 0.091415\n",
            "Train Epoch: 14 [16000/33600 (48%)]\tLoss: 0.003281\n",
            "Train Epoch: 14 [17600/33600 (52%)]\tLoss: 0.004164\n",
            "Train Epoch: 14 [19200/33600 (57%)]\tLoss: 0.011773\n",
            "Train Epoch: 14 [20800/33600 (62%)]\tLoss: 0.007876\n",
            "Train Epoch: 14 [22400/33600 (67%)]\tLoss: 0.034341\n",
            "Train Epoch: 14 [24000/33600 (71%)]\tLoss: 0.027636\n",
            "Train Epoch: 14 [25600/33600 (76%)]\tLoss: 0.013070\n",
            "Train Epoch: 14 [27200/33600 (81%)]\tLoss: 0.155346\n",
            "Train Epoch: 14 [28800/33600 (86%)]\tLoss: 0.005813\n",
            "Train Epoch: 14 [30400/33600 (90%)]\tLoss: 0.015359\n",
            "Train Epoch: 14 [32000/33600 (95%)]\tLoss: 0.339151\n",
            "Train Epoch: 14 [33600/33600 (100%)]\tLoss: 0.018145\n",
            "\n",
            "Average Val Loss: 0.0263, Val Accuracy: 8339/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 15 [1600/33600 (5%)]\tLoss: 0.011912\n",
            "Train Epoch: 15 [3200/33600 (10%)]\tLoss: 0.027761\n",
            "Train Epoch: 15 [4800/33600 (14%)]\tLoss: 0.064433\n",
            "Train Epoch: 15 [6400/33600 (19%)]\tLoss: 0.047780\n",
            "Train Epoch: 15 [8000/33600 (24%)]\tLoss: 0.104155\n",
            "Train Epoch: 15 [9600/33600 (29%)]\tLoss: 0.014907\n",
            "Train Epoch: 15 [11200/33600 (33%)]\tLoss: 0.106288\n",
            "Train Epoch: 15 [12800/33600 (38%)]\tLoss: 0.002135\n",
            "Train Epoch: 15 [14400/33600 (43%)]\tLoss: 0.009316\n",
            "Train Epoch: 15 [16000/33600 (48%)]\tLoss: 0.012298\n",
            "Train Epoch: 15 [17600/33600 (52%)]\tLoss: 0.059838\n",
            "Train Epoch: 15 [19200/33600 (57%)]\tLoss: 0.003669\n",
            "Train Epoch: 15 [20800/33600 (62%)]\tLoss: 0.021582\n",
            "Train Epoch: 15 [22400/33600 (67%)]\tLoss: 0.015055\n",
            "Train Epoch: 15 [24000/33600 (71%)]\tLoss: 0.233090\n",
            "Train Epoch: 15 [25600/33600 (76%)]\tLoss: 0.347270\n",
            "Train Epoch: 15 [27200/33600 (81%)]\tLoss: 0.008906\n",
            "Train Epoch: 15 [28800/33600 (86%)]\tLoss: 0.010253\n",
            "Train Epoch: 15 [30400/33600 (90%)]\tLoss: 0.008453\n",
            "Train Epoch: 15 [32000/33600 (95%)]\tLoss: 0.011667\n",
            "Train Epoch: 15 [33600/33600 (100%)]\tLoss: 0.031293\n",
            "\n",
            "Average Val Loss: 0.0263, Val Accuracy: 8338/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 16 [1600/33600 (5%)]\tLoss: 0.004847\n",
            "Train Epoch: 16 [3200/33600 (10%)]\tLoss: 0.007524\n",
            "Train Epoch: 16 [4800/33600 (14%)]\tLoss: 0.120933\n",
            "Train Epoch: 16 [6400/33600 (19%)]\tLoss: 0.036857\n",
            "Train Epoch: 16 [8000/33600 (24%)]\tLoss: 0.003510\n",
            "Train Epoch: 16 [9600/33600 (29%)]\tLoss: 0.002574\n",
            "Train Epoch: 16 [11200/33600 (33%)]\tLoss: 0.163696\n",
            "Train Epoch: 16 [12800/33600 (38%)]\tLoss: 0.014536\n",
            "Train Epoch: 16 [14400/33600 (43%)]\tLoss: 0.003828\n",
            "Train Epoch: 16 [16000/33600 (48%)]\tLoss: 0.006684\n",
            "Train Epoch: 16 [17600/33600 (52%)]\tLoss: 0.072456\n",
            "Train Epoch: 16 [19200/33600 (57%)]\tLoss: 0.018977\n",
            "Train Epoch: 16 [20800/33600 (62%)]\tLoss: 0.004142\n",
            "Train Epoch: 16 [22400/33600 (67%)]\tLoss: 0.014089\n",
            "Train Epoch: 16 [24000/33600 (71%)]\tLoss: 0.028503\n",
            "Train Epoch: 16 [25600/33600 (76%)]\tLoss: 0.089583\n",
            "Train Epoch: 16 [27200/33600 (81%)]\tLoss: 0.000969\n",
            "Train Epoch: 16 [28800/33600 (86%)]\tLoss: 0.019186\n",
            "Train Epoch: 16 [30400/33600 (90%)]\tLoss: 0.062232\n",
            "Train Epoch: 16 [32000/33600 (95%)]\tLoss: 0.034721\n",
            "Train Epoch: 16 [33600/33600 (100%)]\tLoss: 0.026102\n",
            "\n",
            "Average Val Loss: 0.0280, Val Accuracy: 8338/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 17 [1600/33600 (5%)]\tLoss: 0.234175\n",
            "Train Epoch: 17 [3200/33600 (10%)]\tLoss: 0.069874\n",
            "Train Epoch: 17 [4800/33600 (14%)]\tLoss: 0.009594\n",
            "Train Epoch: 17 [6400/33600 (19%)]\tLoss: 0.006907\n",
            "Train Epoch: 17 [8000/33600 (24%)]\tLoss: 0.009585\n",
            "Train Epoch: 17 [9600/33600 (29%)]\tLoss: 0.363359\n",
            "Train Epoch: 17 [11200/33600 (33%)]\tLoss: 0.034495\n",
            "Train Epoch: 17 [12800/33600 (38%)]\tLoss: 0.052999\n",
            "Train Epoch: 17 [14400/33600 (43%)]\tLoss: 0.029918\n",
            "Train Epoch: 17 [16000/33600 (48%)]\tLoss: 0.206439\n",
            "Train Epoch: 17 [17600/33600 (52%)]\tLoss: 0.004585\n",
            "Train Epoch: 17 [19200/33600 (57%)]\tLoss: 0.200595\n",
            "Train Epoch: 17 [20800/33600 (62%)]\tLoss: 0.051830\n",
            "Train Epoch: 17 [22400/33600 (67%)]\tLoss: 0.000753\n",
            "Train Epoch: 17 [24000/33600 (71%)]\tLoss: 0.122041\n",
            "Train Epoch: 17 [25600/33600 (76%)]\tLoss: 0.006481\n",
            "Train Epoch: 17 [27200/33600 (81%)]\tLoss: 0.129494\n",
            "Train Epoch: 17 [28800/33600 (86%)]\tLoss: 0.010557\n",
            "Train Epoch: 17 [30400/33600 (90%)]\tLoss: 0.021648\n",
            "Train Epoch: 17 [32000/33600 (95%)]\tLoss: 0.013407\n",
            "Train Epoch: 17 [33600/33600 (100%)]\tLoss: 0.077581\n",
            "\n",
            "Average Val Loss: 0.0266, Val Accuracy: 8339/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 18 [1600/33600 (5%)]\tLoss: 0.280215\n",
            "Train Epoch: 18 [3200/33600 (10%)]\tLoss: 0.054665\n",
            "Train Epoch: 18 [4800/33600 (14%)]\tLoss: 0.002498\n",
            "Train Epoch: 18 [6400/33600 (19%)]\tLoss: 0.004523\n",
            "Train Epoch: 18 [8000/33600 (24%)]\tLoss: 0.039859\n",
            "Train Epoch: 18 [9600/33600 (29%)]\tLoss: 0.017585\n",
            "Train Epoch: 18 [11200/33600 (33%)]\tLoss: 0.006042\n",
            "Train Epoch: 18 [12800/33600 (38%)]\tLoss: 0.014114\n",
            "Train Epoch: 18 [14400/33600 (43%)]\tLoss: 0.036202\n",
            "Train Epoch: 18 [16000/33600 (48%)]\tLoss: 0.423191\n",
            "Train Epoch: 18 [17600/33600 (52%)]\tLoss: 0.008081\n",
            "Train Epoch: 18 [19200/33600 (57%)]\tLoss: 0.070070\n",
            "Train Epoch: 18 [20800/33600 (62%)]\tLoss: 0.018026\n",
            "Train Epoch: 18 [22400/33600 (67%)]\tLoss: 0.322561\n",
            "Train Epoch: 18 [24000/33600 (71%)]\tLoss: 0.082417\n",
            "Train Epoch: 18 [25600/33600 (76%)]\tLoss: 0.030112\n",
            "Train Epoch: 18 [27200/33600 (81%)]\tLoss: 0.005117\n",
            "Train Epoch: 18 [28800/33600 (86%)]\tLoss: 0.317425\n",
            "Train Epoch: 18 [30400/33600 (90%)]\tLoss: 0.326729\n",
            "Train Epoch: 18 [32000/33600 (95%)]\tLoss: 0.006199\n",
            "Train Epoch: 18 [33600/33600 (100%)]\tLoss: 0.318391\n",
            "\n",
            "Average Val Loss: 0.0268, Val Accuracy: 8339/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 19 [1600/33600 (5%)]\tLoss: 0.261800\n",
            "Train Epoch: 19 [3200/33600 (10%)]\tLoss: 0.014155\n",
            "Train Epoch: 19 [4800/33600 (14%)]\tLoss: 0.002410\n",
            "Train Epoch: 19 [6400/33600 (19%)]\tLoss: 0.002394\n",
            "Train Epoch: 19 [8000/33600 (24%)]\tLoss: 0.136966\n",
            "Train Epoch: 19 [9600/33600 (29%)]\tLoss: 0.009371\n",
            "Train Epoch: 19 [11200/33600 (33%)]\tLoss: 0.020218\n",
            "Train Epoch: 19 [12800/33600 (38%)]\tLoss: 0.006213\n",
            "Train Epoch: 19 [14400/33600 (43%)]\tLoss: 0.023881\n",
            "Train Epoch: 19 [16000/33600 (48%)]\tLoss: 0.004134\n",
            "Train Epoch: 19 [17600/33600 (52%)]\tLoss: 0.024180\n",
            "Train Epoch: 19 [19200/33600 (57%)]\tLoss: 0.025427\n",
            "Train Epoch: 19 [20800/33600 (62%)]\tLoss: 0.157015\n",
            "Train Epoch: 19 [22400/33600 (67%)]\tLoss: 0.025922\n",
            "Train Epoch: 19 [24000/33600 (71%)]\tLoss: 0.017022\n",
            "Train Epoch: 19 [25600/33600 (76%)]\tLoss: 0.121109\n",
            "Train Epoch: 19 [27200/33600 (81%)]\tLoss: 0.085727\n",
            "Train Epoch: 19 [28800/33600 (86%)]\tLoss: 0.058507\n",
            "Train Epoch: 19 [30400/33600 (90%)]\tLoss: 0.290803\n",
            "Train Epoch: 19 [32000/33600 (95%)]\tLoss: 0.007737\n",
            "Train Epoch: 19 [33600/33600 (100%)]\tLoss: 0.170789\n",
            "\n",
            "Average Val Loss: 0.0274, Val Accuracy: 8339/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 20 [1600/33600 (5%)]\tLoss: 0.013105\n",
            "Train Epoch: 20 [3200/33600 (10%)]\tLoss: 0.003111\n",
            "Train Epoch: 20 [4800/33600 (14%)]\tLoss: 0.000506\n",
            "Train Epoch: 20 [6400/33600 (19%)]\tLoss: 0.086139\n",
            "Train Epoch: 20 [8000/33600 (24%)]\tLoss: 0.038995\n",
            "Train Epoch: 20 [9600/33600 (29%)]\tLoss: 0.222901\n",
            "Train Epoch: 20 [11200/33600 (33%)]\tLoss: 0.039129\n",
            "Train Epoch: 20 [12800/33600 (38%)]\tLoss: 0.017007\n",
            "Train Epoch: 20 [14400/33600 (43%)]\tLoss: 0.014040\n",
            "Train Epoch: 20 [16000/33600 (48%)]\tLoss: 0.006657\n",
            "Train Epoch: 20 [17600/33600 (52%)]\tLoss: 0.439335\n",
            "Train Epoch: 20 [19200/33600 (57%)]\tLoss: 0.002226\n",
            "Train Epoch: 20 [20800/33600 (62%)]\tLoss: 0.001110\n",
            "Train Epoch: 20 [22400/33600 (67%)]\tLoss: 0.092856\n",
            "Train Epoch: 20 [24000/33600 (71%)]\tLoss: 0.045186\n",
            "Train Epoch: 20 [25600/33600 (76%)]\tLoss: 0.070443\n",
            "Train Epoch: 20 [27200/33600 (81%)]\tLoss: 0.044252\n",
            "Train Epoch: 20 [28800/33600 (86%)]\tLoss: 0.311246\n",
            "Train Epoch: 20 [30400/33600 (90%)]\tLoss: 0.017690\n",
            "Train Epoch: 20 [32000/33600 (95%)]\tLoss: 0.031419\n",
            "Train Epoch: 20 [33600/33600 (100%)]\tLoss: 0.000892\n",
            "\n",
            "Average Val Loss: 0.0255, Val Accuracy: 8342/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 21 [1600/33600 (5%)]\tLoss: 0.016262\n",
            "Train Epoch: 21 [3200/33600 (10%)]\tLoss: 0.071186\n",
            "Train Epoch: 21 [4800/33600 (14%)]\tLoss: 0.122802\n",
            "Train Epoch: 21 [6400/33600 (19%)]\tLoss: 0.094515\n",
            "Train Epoch: 21 [8000/33600 (24%)]\tLoss: 0.219491\n",
            "Train Epoch: 21 [9600/33600 (29%)]\tLoss: 0.077850\n",
            "Train Epoch: 21 [11200/33600 (33%)]\tLoss: 0.139424\n",
            "Train Epoch: 21 [12800/33600 (38%)]\tLoss: 0.061610\n",
            "Train Epoch: 21 [14400/33600 (43%)]\tLoss: 0.006105\n",
            "Train Epoch: 21 [16000/33600 (48%)]\tLoss: 0.017305\n",
            "Train Epoch: 21 [17600/33600 (52%)]\tLoss: 0.432202\n",
            "Train Epoch: 21 [19200/33600 (57%)]\tLoss: 0.123008\n",
            "Train Epoch: 21 [20800/33600 (62%)]\tLoss: 0.092013\n",
            "Train Epoch: 21 [22400/33600 (67%)]\tLoss: 0.055140\n",
            "Train Epoch: 21 [24000/33600 (71%)]\tLoss: 0.443095\n",
            "Train Epoch: 21 [25600/33600 (76%)]\tLoss: 0.227101\n",
            "Train Epoch: 21 [27200/33600 (81%)]\tLoss: 0.000690\n",
            "Train Epoch: 21 [28800/33600 (86%)]\tLoss: 0.071723\n",
            "Train Epoch: 21 [30400/33600 (90%)]\tLoss: 0.016136\n",
            "Train Epoch: 21 [32000/33600 (95%)]\tLoss: 0.002544\n",
            "Train Epoch: 21 [33600/33600 (100%)]\tLoss: 0.050071\n",
            "\n",
            "Average Val Loss: 0.0260, Val Accuracy: 8342/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 22 [1600/33600 (5%)]\tLoss: 0.073690\n",
            "Train Epoch: 22 [3200/33600 (10%)]\tLoss: 0.008976\n",
            "Train Epoch: 22 [4800/33600 (14%)]\tLoss: 0.126639\n",
            "Train Epoch: 22 [6400/33600 (19%)]\tLoss: 0.263545\n",
            "Train Epoch: 22 [8000/33600 (24%)]\tLoss: 0.025079\n",
            "Train Epoch: 22 [9600/33600 (29%)]\tLoss: 0.019790\n",
            "Train Epoch: 22 [11200/33600 (33%)]\tLoss: 0.143183\n",
            "Train Epoch: 22 [12800/33600 (38%)]\tLoss: 0.001048\n",
            "Train Epoch: 22 [14400/33600 (43%)]\tLoss: 0.004309\n",
            "Train Epoch: 22 [16000/33600 (48%)]\tLoss: 0.317279\n",
            "Train Epoch: 22 [17600/33600 (52%)]\tLoss: 0.013585\n",
            "Train Epoch: 22 [19200/33600 (57%)]\tLoss: 0.016827\n",
            "Train Epoch: 22 [20800/33600 (62%)]\tLoss: 0.010510\n",
            "Train Epoch: 22 [22400/33600 (67%)]\tLoss: 0.036246\n",
            "Train Epoch: 22 [24000/33600 (71%)]\tLoss: 0.008416\n",
            "Train Epoch: 22 [25600/33600 (76%)]\tLoss: 0.007946\n",
            "Train Epoch: 22 [27200/33600 (81%)]\tLoss: 0.014950\n",
            "Train Epoch: 22 [28800/33600 (86%)]\tLoss: 0.407111\n",
            "Train Epoch: 22 [30400/33600 (90%)]\tLoss: 0.059018\n",
            "Train Epoch: 22 [32000/33600 (95%)]\tLoss: 0.087349\n",
            "Train Epoch: 22 [33600/33600 (100%)]\tLoss: 0.017071\n",
            "\n",
            "Average Val Loss: 0.0273, Val Accuracy: 8340/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 23 [1600/33600 (5%)]\tLoss: 0.068779\n",
            "Train Epoch: 23 [3200/33600 (10%)]\tLoss: 0.003790\n",
            "Train Epoch: 23 [4800/33600 (14%)]\tLoss: 0.001154\n",
            "Train Epoch: 23 [6400/33600 (19%)]\tLoss: 0.011278\n",
            "Train Epoch: 23 [8000/33600 (24%)]\tLoss: 0.244921\n",
            "Train Epoch: 23 [9600/33600 (29%)]\tLoss: 0.005675\n",
            "Train Epoch: 23 [11200/33600 (33%)]\tLoss: 0.027250\n",
            "Train Epoch: 23 [12800/33600 (38%)]\tLoss: 0.004653\n",
            "Train Epoch: 23 [14400/33600 (43%)]\tLoss: 0.458570\n",
            "Train Epoch: 23 [16000/33600 (48%)]\tLoss: 0.057173\n",
            "Train Epoch: 23 [17600/33600 (52%)]\tLoss: 0.006924\n",
            "Train Epoch: 23 [19200/33600 (57%)]\tLoss: 0.019123\n",
            "Train Epoch: 23 [20800/33600 (62%)]\tLoss: 0.030094\n",
            "Train Epoch: 23 [22400/33600 (67%)]\tLoss: 0.020039\n",
            "Train Epoch: 23 [24000/33600 (71%)]\tLoss: 0.154406\n",
            "Train Epoch: 23 [25600/33600 (76%)]\tLoss: 0.078959\n",
            "Train Epoch: 23 [27200/33600 (81%)]\tLoss: 0.088413\n",
            "Train Epoch: 23 [28800/33600 (86%)]\tLoss: 0.013743\n",
            "Train Epoch: 23 [30400/33600 (90%)]\tLoss: 0.295957\n",
            "Train Epoch: 23 [32000/33600 (95%)]\tLoss: 0.008033\n",
            "Train Epoch: 23 [33600/33600 (100%)]\tLoss: 0.022078\n",
            "\n",
            "Average Val Loss: 0.0268, Val Accuracy: 8340/8400 (99.000%)\n",
            "\n",
            "Train Epoch: 24 [1600/33600 (5%)]\tLoss: 0.223474\n",
            "Train Epoch: 24 [3200/33600 (10%)]\tLoss: 0.076882\n",
            "Train Epoch: 24 [4800/33600 (14%)]\tLoss: 0.002250\n",
            "Train Epoch: 24 [6400/33600 (19%)]\tLoss: 0.012707\n",
            "Train Epoch: 24 [8000/33600 (24%)]\tLoss: 0.011848\n",
            "Train Epoch: 24 [9600/33600 (29%)]\tLoss: 0.104290\n",
            "Train Epoch: 24 [11200/33600 (33%)]\tLoss: 0.003298\n",
            "Train Epoch: 24 [12800/33600 (38%)]\tLoss: 0.012174\n",
            "Train Epoch: 24 [14400/33600 (43%)]\tLoss: 0.020203\n",
            "Train Epoch: 24 [16000/33600 (48%)]\tLoss: 0.087024\n",
            "Train Epoch: 24 [17600/33600 (52%)]\tLoss: 0.002062\n",
            "Train Epoch: 24 [19200/33600 (57%)]\tLoss: 0.010323\n",
            "Train Epoch: 24 [20800/33600 (62%)]\tLoss: 0.058927\n",
            "Train Epoch: 24 [22400/33600 (67%)]\tLoss: 0.087779\n",
            "Train Epoch: 24 [24000/33600 (71%)]\tLoss: 0.033489\n",
            "Train Epoch: 24 [25600/33600 (76%)]\tLoss: 0.052300\n",
            "Train Epoch: 24 [27200/33600 (81%)]\tLoss: 0.061901\n",
            "Train Epoch: 24 [28800/33600 (86%)]\tLoss: 0.009887\n",
            "Train Epoch: 24 [30400/33600 (90%)]\tLoss: 0.009175\n",
            "Train Epoch: 24 [32000/33600 (95%)]\tLoss: 0.079012\n",
            "Train Epoch: 24 [33600/33600 (100%)]\tLoss: 0.048580\n",
            "\n",
            "Average Val Loss: 0.0270, Val Accuracy: 8336/8400 (99.000%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J_RqDraEdz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions(data_loader):\n",
        "    conv_model.eval()\n",
        "    test_preds = torch.LongTensor()\n",
        "    \n",
        "    for i, data in enumerate(data_loader):\n",
        "        data = data.unsqueeze(1)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            data = data.cuda()\n",
        "            \n",
        "        output = conv_model(data)\n",
        "        \n",
        "        preds = output.cpu().data.max(1, keepdim=True)[1]\n",
        "        test_preds = torch.cat((test_preds, preds), dim=0)\n",
        "        \n",
        "    return test_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rJRKGq7W1Pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set_preds = make_predictions(test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVsrzYi6W3hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df = pd.read_csv(\"/content/drive/My Drive/Data Set/sample_submission.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX-yWY4dXByO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7f81b42c-7735-4926-9fb9-3245b33bf0b7"
      },
      "source": [
        "submission_df['Label'] = test_set_preds.numpy().squeeze()\n",
        "submission_df.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ImageId</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ImageId  Label\n",
              "0        1      2\n",
              "1        2      0\n",
              "2        3      9\n",
              "3        4      9\n",
              "4        5      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VuM7xU7XDzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SLNV153XF1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvJpoRMGXeMF",
        "colab_type": "text"
      },
      "source": [
        "Achieved an accuracy of 0.99271% on Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2OXPPiiXjB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}